{
  "metadata": {
    "filename": "temp_0194be08-5359-74ba-aa8c-78ab261f8b00",
    "extraction_date": "2025-01-31T20:24:17.798942",
    "path": "research_assistant\\data\\temp_0194be08-5359-74ba-aa8c-78ab261f8b00.pdf"
  },
  "pages": {
    "1": "Software Testing in Open Innovation: An Exploratory Case\nStudy of the Acceptance Test Harness for Jenkins\nHussan Munir\nLund University, Sweden\nhussan.munir@cs.lth.se\nPer Runeson\nLund University, Sweden\nper.runeson@cs.lth.se\nABSTRACT\nOpen Innovation (OI) has gained signiﬁcant attention since\nthe term was introduced in 2003. However, little is known\nwhether general software testing processes are well suited for\nOI. An exploratory case study on the Acceptance Test Har-\nness (ATH) is conducted to investigate OI testing activities\nof Jenkins. As far as the research methodology is concerned,\nwe extracted the change log data of ATH followed by ﬁve\ninterviews with key contributors in the development of ATH.\nThe ﬁndings of the study are threefold. First, it highlights\nthe key stakeholders involved in the development of ATH.\nSecond, the study compares the ATH testing activities with\nISO/IEC/IEEE testing process and presents a tailored pro-\ncess for software testing in OI. Finally, the study underlines\nsome key challenges that software intensive organizations\nface while working with the testing in OI.\nCategories and Subject Descriptors\nD.2.5 [Testing and Debugging]: [testing tool]\nKeywords\nTesting, Open Innovation, OSS, Acceptance Test Harness,\nJenkins\n1.\nINTRODUCTION\nOpen Innovation (OI) is an emerging paradigm in Software\nEngineering (SE) which lacks empirical evidence for software-\nintensive organizations. In 2003, Chesbrough deﬁned OI\nas follows [6]: “A paradigm that assumes that ﬁrms can\nand should use external ideas as well as internal ideas, and\ninternal and external path to markets, as they look to advance\ntheir technology”.\nOne of the most well known ways for\nenabling OI in the software-intensive organizations is the use\nof Open Source Software (OSS). However, it is important to\nacknowledge that OI and OSS are not the same. In order for\nOSS to be used as an example of OI, ﬁrms’ OSS activities\nmust be in line with their business model to create and\ncapture value. OI is more transactional in nature, compared\nto OSS, where ﬁrms try to leverage external knowledge to\naccelerate their internal innovation process and in return,\ncontribute back to the community by adopting a selective\nrevealing strategy [8].\nPrior to this study, we conducted a systematic mapping\nstudy [11] on OI in SE to identify the research in the ﬁeld.\nThe study shows that empirical studies on the role of testing\nin OI are scarce. Furthermore, software testing in OI entails\na dual role: 1) to verify the functions and characteristics of\nopen components and services, supplied by the community,\nand 2) to verify the functions and characteristics of services\ndelivered to stakeholder higher up in the value chain (e.g.\ninternal customers, software developers and testers). Further-\nmore, it is still unknown whether or not the general practices\nof software testing are feasible to deal with the challenges of\nOI.\nThis paves way for this exploratory case study with the\nmain focus of software testing in OI. The object of study\nis Jenkins, an open source build server [2]. The main ob-\njective of this study is to identify the top contributors to\nthe Acceptance Test Harness (ATH) which is part of the\nJenkins project (see Section 3.1) and explore the testing\nprocesses used to test Jenkins, using ATH. Furthermore, this\npaper presents the key challenges faced by one of the key\ncontributors to Jenkins (see Section 3.3).\n2.\nRESEARCH DESIGN\nIn order to explore software testing activities in OI, we\nlaunched a case study [13] of an OSS project, studied as an\ninstance of OI. The focus of the study is on the initiation\nand development of the ATH to test Jenkins in an auto-\nmated fashion. We conducted the following steps: ﬁrst we\nmined the ATH source code repository and extracted the\nchange log data, using CVSAnly, to characterize the top\nATH contributors in the Jenkins community. Then, face\nto face semi-structured interviews were conducted with the\nkey software developers of ATH (see Table 1). Thirdly, we\nanalyzed the OI testing by relating the process to a general\ntest process, and identiﬁed key challenges for OI testing.\n2.1\nResearch Questions\nOur general interest in understanding OI aspects of soft-\nware testing are detailed in three research questions:\nRQ1: Who are the top stakeholders involved in the devel-\nopment of ATH and are those stakeholders the same\nas the contributors of Jenkins?\nThis is the author’s version of the work. It is posted here for your personal use. Not for\nredistribution. The deﬁnitive version was published in the following publication:\nICSSP’15, August 24–26, 2015, Tallinn, Estonia\nc⃝2015 ACM. 978-1-4503-3346-7/15/08...\nhttp://dx.doi.org/10.1145/2785592.2795365\n187",
    "2": "RQ2: Do ATH testing activities adheres to ISO/IEC/IEEE\n29119 testing standard?\nRQ3: What are the key challenges associated with testing\nin OI?\nAs a point of reference for general test processes, we used the\nISO/IEC/IEEE 29119 testing standard to compare the ATH\ntesting process with (RQ2). This lead to identifying the key\ncontributors (RQ1) in ATH, followed by interviews with the\ndevelopers of ATH to ﬁnd out the possible hurdles (RQ3)\nin the process. To be more speciﬁc, RQ1 is formulated to\ninvestigate the homogeneity of the community, to see whether\nor not the top contributors of ATH are the same as for the\ncore of Jenkins. RQ1 is answered by mining the change log\ndata from the online ATH GitHub repository. It is to be\nnoted that the change log mining is also used to identify\nthe key contributors of ATH for interviews. RQ2 aims at\nexploring the adherence of ATH to ISO/IEC/IEEE 29119\ntesting standard. In order to analyze the OI test process, the\nISO/IEC/IEEE standard is a reference model for traditional\ntesting. Abdou et al. [3] used ISO/IEC/IEEE standard\nto compare it with the OSS testing process. However, we\nfollowed ISO/IEC/IEEE standard to compare it with the\nOI testing process. There are number of standards available\nfor testing processes, such as ISO/IEC TR 19759, BS 7925-2\nand ISO/IEC WD 29119-2 [1, 4, 12, 3], while we used the\nlatter since is is quite recent and internationally accepted\nas as standard.\nISO/IEC/IEEE 29119 supports various\nsoftware development life cycles including spiral, waterfall\nand agile models. Unlike all previously mentioned models, it\ncovers non functional testing, risk based testing and static\ntesting. The change log data extracted to answer RQ1 helped\nus identifying the key interviewees in order to explore the\nchallenges associated with ATH. RQ3 is used to explore the\nkey challenges associated with OI in SE. RQ2 and RQ3 are\nanswered using interviews with the key contributors from\nthe Jenkins community.\n2.2\nCase Selection and Unit of Analysis\nJenkins is the leading open source continuous integration\nserver [2]. It oﬀers more than 1000 plugins to support build-\ning and testing virtually any project built in Java. These\ntests can be also run with speciﬁc version of the Jenkins core\nand a combination of plugins. Over the passage of time, the\nnumber of test cases for Jenkins has steadily increased to over\n300. In order to test Jenkins with automated tests, one of\nthe key contributors (referring to an anonymous company’s\nemployee) initiated the ATH together with the community.\nATH consists of a reusable harness that can be used by\nplugin developers and users to write functional test cases.\nThese test cases can be run against Jenkins plugins that are\ndeployed in diﬀerent conﬁgurations. The focus of this case\nstudy is to explore OI activities in ATH from the companies’\nperspective rather than community’s perspective.\n3.\nRESULTS AND ANALYSIS\nThis section presents the ﬁndings from the change log\ndata analysis and the semi-structured interviews with the\ncontributors of ATH.\n3.1\nStakeholders in ATH\nThe associations of key contributors were identiﬁed using\ntheir email addresses followed by checking their public proﬁles\nFigure 1: Key contributors to the Acceptance Test\nHarness.\non GitHub. As can be seen in Fig. 1, Cloudbees and Redhat\nare the biggest contributors to the development of ATH.\nSurprisingly, the third biggest contributor turned out to\nbe Munich University which is an indication of strong ties\nbetween the Jenkins community and academia [5].\nIt is\nto be noted that the biggest contributor in Jenkins is not\namong the top for ATH. However, we chose to interview key\ncontributors to Jenkins, since they initiated the idea of ATH\nand convinced the community to start testing Jenkins in an\nautomated fashion.\nInterview data suggests that the testing process of Jenkins\nis the least attended process since the community was using\nan old manual approach for the testing of Jenkins. Openness\nlead them to consider switching from the manual to an auto-\nmated testing process. Initially, the idea of testing Jenkins\nin an automated fashion came from the community. An\ninterviewee 3 stated, “. . . the idea of acceptance test harness\ncame from the community but [our company] was the biggest\ncontributor to actually getting traction on it”.\nSelenium is a test harness that tests Jenkins from outside,\nusing automated tests. Originally it was written in Ruby,\nwhile Jenkins generally have unit test cases written in HTML\nthat only test a particular plug-in in a unit test manner.\nIn 2011, this became a signiﬁcant problem for two reasons.\nFirstly, the HTML solution did not scale up to a large number\nof plug-ins, and secondly, the community primarily used Java,\nand thus the Ruby implementation became a bottleneck\nregarding competence.\nTherefore, the main contributor took on the scaleability\nissue and together with the Jenkins community decided to\nrework the test harness into Java. The ATH is comprised\nof a reusable harness that can be used by users and plug-in\ndevelopers to write functional test cases. An interviewee\n3 stated that, “. . . from [the company’s] perspective, we can\ncontribute our internal acceptance test cases to the community\nand have the community actually to execute those tests when\nits time to test a new stable version and upgrading. Similarly,\nit helps other companies and the community to do the same.\nTherefore, it’s a tool that helps everyone”.\nThe programming language (Ruby) and scaleability issues\nof the Selenium(HTML) test harness came up as a bottle-\nneck to adopt it for the Jenkins testing, since the Jenkins\ncommunity works with Java. Therefore, the ATH is created\nto port selenium test cases into Java unit test cases in order\nto test the Jenkins through automated acceptance test cases.\n188",
    "3": "Table 1: Interviewees description\nAnonymous name\nInvolvement\nExperience\nRole\nInterviewee 1\nJenkins and ATH\n8 Years\nTools manager for Jenkins and contributing\nto ATH\nInterviewee 2\nJenkins and and ATH\n6 Years\nTeam lead and ATH contributor\nInterviewee 3\nJenkins\n7 Years\nFormer tools manager Jenkins\nInterviewee 4\nResponsible for Jenkins and Gerrit build\nartifacts and channel distribution\n8 years\nSoftware Architect\nInterviewee 5\nOSS policy maker\nMore than 20 Years\nManager responsible for overall OSS strat-\negy\nTable 2: Diﬀerences between ISO/IEC/IEEE 29119 and the OI testing process\nPhases\nISO/IEC/IEEE Test Process\nAcceptance test harness frame-\nwork\nTest Design and Implementation\n• Identify Feature Sets\n• Derive Test Conditions\n• Derive Test Coverage Items\n• Derive Test Cases\n• Assemble Test Sets\n• Derive Test Procedures\n• Identify Independently\n• Derive Testable Aspect(s)\n• Derive Test Cases\n• Documentation\nTest Environment Set-Up and Mainte-\nnance\n• Establish Test Environment\n• Maintain Test Environment\n• Select Test Environment\nTest Execution\n• Execute Test Procedure(s)\n• Compare Test Results\n• Record Test Execution\n• Test thoroughness depends on\ndevelopers expertise\n• Execute\n• Submit\nTest Incident Reporting\n• Analyze Test Result(s)\n• Create Incident Report\n• Review or Vote\n• Accept\n• Console reports\n• Textﬁle reports\n3.2\nThe ATH Testing Process\nIn this section we compare the ATH testing process to the\nISO/IEC/IEEE 29119 testing standard. The structure below\nfollow the main phases of the standard.\n3.2.1\nTest Design and Implementation\nThe OI testing is less formalized than described in the\nstandard. ATH does not have an explicit test plan and there\nis no formal identiﬁcation of risks (see Fig. 2). The features\nto be tested are prioritized according to the community’s or\ncontributor’s internal needs independently without consulting\nother stakeholders in the project. Testable objects (plugins)\nare based on the speciﬁc interests from volunteers who to\nchoose take over the testing for a component (see Table 2).\n3.2.2\nTest Environment Set-Up and Maintenance\nAccording to the ISO/IEC standard, test cases are derived\nby determining the pre-conditions, post-conditions, input\nvalues and expected outcomes. However, in the absence of a\ntest plan, ATH does not have test completion criteria that\nmeasure the test coverage of the Jenkins plugins. An intervie-\nwee 1 stated that, “. . . the existence of acceptance test cases\ndoes not guarantee that Jenkins would not blow up every time\nwe upgrade the core or its plugins since it is so volatile with\nwith diﬀerent plugins and conﬁgurations. However, it can\ngive us a fair indication what went wrong and enable to ﬁx\nthe problem.’. In most OSS projects, testers move directly\nto test execution from test design and implementation in\norder to execute test procedures, since there are no speciﬁc\nrequirements for the test environment in ATH (see Fig. 2).\nHowever, ATH has an abstraction called JenkinsController\nthat allows using diﬀerent logic for starting/stopping Jenk-\nins. It is used to run the same set of tests against many\ndiﬀerent ways of launching Jenkins, such as through Java,\nJBoss, a Debian package, etc. To select a controller, run the\ntest with the TYPE environment variable set to the con-\ntroller ID. Common conﬁguration of controllers can be done\nthrough environment variables, and the following controllers\nare available in ATH.\n• Winstone controller (TYPE=winstone)\n• Winstone Docker controller (TYPE=winstone docker)\n• Existing Jenkins’ controller (TYPE=existing)\n189",
    "4": "Figure 2: The ATH testing process\n• Tomcat controller (TYPE=tomcat)\n• JBoss controller (TYPE=jboss)\n• Ubuntu controller (TYPE=ubuntu)\n• CentOS controller (TYPE=centos)\n• OpenSUSE controller (TYPE=opensuse)\nDue to the open plug-in nature of the Jenkins it is dif-\nﬁcult to have a one controller for all the above mentioned\nconﬁgurations. Therefore, the ATH provides diﬀerent types\nof controllers to run the acceptance test suite against many\ndiﬀerent environments by setting the environment variable.\n3.2.3\nTest Execution\nThe Test Execution process generally begins with a devel-\noper, testing the local copy of Jenkins after checkout and\nthe thoroughness of test cases depends on the expertise and\njudgment of the developer (see Fig. 2). However, in order to\nensure the quality of acceptance test cases, a video tutorial is\navailable on the Jenkins web page that shows how to write an\nacceptance test case. ATH uses WebDriver to execute tests,\nand the developers have the option to choose the browser by\nusing the BROWSER environment variable. The following\nbrowser variables are compatible with ATH to execute the\ntest suite.\n• ﬁrefox (default)\n• ie\n• chrome\n• safari\n• htmlunit\n• phantomjs\n3.2.4\nTest Incident Reporting\nTest incident reporting in ATH depends on the discussions\nin the mailing lists, which solicit feedback from interested\nstakeholders, and the voting system. ATH allows Console\nand Textﬁle reports. The Console Reporter logs the plugin\nand its version to standard output. The Textﬁle Reporter\ncreates a properties ﬁle in the target folder containing a list\nof plugin names and their versions, preﬁxed by the test name.\nIt can be very useful when users want to be able to see which\nplugins and their versions that were tested with a particular\nversion of Jenkins Core. Note that the reporting can be\nperformed by any stakeholder involved in the development\nof ATH. However, the community uses the voting system\nto prioritize the most critical bugs (see Table 2). Moreover,\ntest results are categorized into three categories: 1) OK, 2)\nWarn, and 3) Fail. Test results are considered OK if the\ntests passed without any problems. Warning indicates a\nfunctionality malfunction, but it does not cause the whole\nJenkins to fail. On the contrary, failure indicates a fatal error\nof the whole Jenkins or a critical issue that aﬀect all jobs,\ni.e. developers have not been to start Jenkins or save the\nconﬁguration of any job.\nInterviewees stated that external bugs are reported through\nthe JIRA tool and internal bugs are reported using emails.\nThe prioritization is based on the most pressing needs of\nsoftware developers at the contributor. Additionally, all the\ntest cases are written in Java using JUnit. An interviewee 3\nstated, “. . . if it is sort of a make or break we ﬁx it ourselves\nand then make a pull request and if its not, we report it.\nMay be someone from the community in the future will ﬁx\nit”. Other stakeholders in the ATH also do the same, by\ntaking the most important issues according to their needs\nand ﬁx them ﬁrst. However, it is important to show to the\ncommunity that we want to contribute to the project as a\nwhole and not just our part, as mentioned by Dahlander [7].\n190",
    "5": "An interviewee 3 stated “. . . sometimes, if we have a big issue\nsomeone else may have it too and we can focus on ﬁxing\nother bugs so, we try to forward as many issues as possible.”.\nThis ﬁnding is inline with the ﬁndings of Henkel et al. [9].\n3.3\nSoftware Testing Challenges in OI\nOne of the biggest testing challenges is to have a complete\ncoverage as there are many diﬀerent conﬁgurations and setups\navailable due to the open plug-in nature of Jenkins. As one\ninterviewee 1 stated “. . . Jenkins is composed of so many\nsmall plugins thereby, every time [company] upgrade core and\nplugins for master services it blows up”. Therefore, running\nthe ATH test suites before applying an upgrade gives the\nteam an indication about what might fail during the process.\nIn addition to this quote, the interviewee further elaborated\nthat they have some acceptance tests and the test suite is\ngetting bigger, and therefore urges for a need to set up an\nautomatic building (ATH) for every patch set. The second\nmajor challenge is related to the availability of resources, as\nan interviewee 3 stated“. . . the hot shots in the community are\nreally busy and do not have enough time to take on some of\nthe most daring challenges we face. Therefore, sometimes it is\nfrustrating to get an answer from the community quickly.” as\nconﬁrmed by Morgan et al. [10]. This also traces back to our\ncomparison (see Table 2) where stakeholders prioritize and ﬁx\ntheir issues independently, without taking other stakeholders\ninto account.\n4.\nCONCLUSIONS\nThis case study explored testing activities in Jenkins, using\nATH and compared the testing activities of ATH with the\nISO/IEC/IEEE testing standard. We extracted the change\nlog data of ATH in order to identify the major contributors\n(RQ1). In the interviews, we found out that although the\ninitial idea of ATH came from the community, the major\nJenkins contributor brought ATH to the community’s at-\ntention at hackathons. Additionally, along with Cloudbees\nand Redhat, Munich university came out as a third biggest\ncontributor, which suggests strong ties between the Jenkins\ncommunity and industry. Further, we compare the diﬀerence\nbetween the ATH testing process and the ISO/IEC/IEEE\ntesting standard (see Table 2). The key diﬀerence is that\nthe ATH does not have a test plan and therefore, there is\nno test completion criteria that measures the test coverage\nof all Jenkins plugins. The thoroughness of the acceptance\ntest cases depends on the subjective judgment of developers\n(RQ2). Finally, we identiﬁed key challenges for the testing\nprocess in OI (RQ3). For example, due to the huge number\n(1000+) of open Jenkins plugins and its nature with many\nsettings, it is a daunting task to have a complete test coverage.\nHowever, the ATH makes the whole Jenkins defect detection\nprocess better by pointing to defects in faulty plugins and\nthereby, enables developers to take corrective actions more\neﬃciently.\nIt can be concluded that the ATH testing process does\nnot strictly adhere to the ISO/IEC/IEEE testing standard\nbecause testable features are identiﬁed by software engineers\nindependently without any formal test plan. The test cover-\nage is dependent on software engineers subjective judgment\nand hence, it is very diﬃcult to achieve the complete test\ncoverage. Furthermore, diﬀerent controllers to run accep-\ntance test cases against many diﬀerent environments is an\nindication that the standard Software Engineering testing\nprocess needs to be adapted to deal with challenges of OI.\nFinally, it is worth mentioning that Jenkins is a development\ninfrastructure and ATH is used to test this infrastructure. In\neﬀect, ATH may not necessarily a representative of regular\nsoftware example.\n5.\nREFERENCES\n[1] ISO/IEC/IEEE 29119 Software Testing Standard.\n[2] The Jenkins Gerrit trigger plugin open source project\non Ohloh. Accessed: 2014-07-08.\n[3] T. Abdou, P. Grogono, and P. Kamthan. A conceptual\nframework for open source software test process. In\n36th Annual Computer Software and Applications\nConference Workshops (COMPSACW), pages 458–463,\nJuly 2012.\n[4] P. Bourque, R. Dupuis, A. Abran, J. Moore, and\nL. Tripp. The guide to the Software Engineering Body\nof Knowledge. IEEE Software, 16(6):35–44, Nov. 1999.\n[5] H. Chesbrough, W. Vanhaverbeke, and J. West. Open\ninnovation: Researching a new paradigm. Oxford\nuniversity press, 2006.\n[6] H. W. Chesbrough. Open innovation: the new\nimperative for creating and proﬁting from technology.\nHarvard Business School Press, Boston, Mass., 2003.\n[7] L. Dahlander and M. W. Wallin. A man on the inside:\nUnlocking communities as complementary assets.\nResearch Policy, 35(8):1243–1259, 2006.\n[8] J. Henkel. Selective revealing in open innovation\nprocesses: The case of embedded linux. Research\nPolicy, 35(7):953–969, 2006.\n[9] J. Henkel. Champions of revealing-the role of open\nsource developers in commercial ﬁrms. Industrial and\nCorporate Change, 18(3):435–471, Dec. 2008.\n[10] L. Morgan and P. Finnegan. Open innovation in\nsecondary software ﬁrms: An exploration of managers\nperceptions of open source software. SIGMIS Database,\n41(1):76–95, 2010.\n[11] H. Munir, K. Wnuk, and P. Runeson. Open innovation\nin software engineering: a systematic mapping study.\nEmpirical Software Engineering, 2015.\n[12] S. Reid. BS 7925-2: the software component testing\nstandard. In Proceedings First Asia-Paciﬁc Conference\non Quality Software, pages 139–148, 2000.\n[13] P. Runeson, M. H¨ost, A. Rainer, and B. Regnell. Case\nStudy Research in Software Engineering - Guidelines\nand Examples. Wiley, 2012.\n191"
  },
  "tables": {},
  "images": {
    "2": [
      {
        "filename": "page_2_image_1.png",
        "path": "output\\images\\temp_0194be08-5359-74ba-aa8c-78ab261f8b00\\page_2_image_1.png",
        "extraction_date": "2025-01-31T20:24:18.557741",
        "page_number": 2,
        "image_index": 1,
        "width": 748,
        "height": 430
      }
    ],
    "4": [
      {
        "filename": "page_4_image_1.png",
        "path": "output\\images\\temp_0194be08-5359-74ba-aa8c-78ab261f8b00\\page_4_image_1.png",
        "extraction_date": "2025-01-31T20:24:18.575524",
        "page_number": 4,
        "image_index": 1,
        "width": 807,
        "height": 469
      }
    ]
  }
}