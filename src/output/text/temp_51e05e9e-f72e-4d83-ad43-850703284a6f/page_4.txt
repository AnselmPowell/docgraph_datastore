Start by issuing the docker search mysql command, which then displays a list of images in the public Docker registry that match the keyword “mysql”. For no particular
reason other than I know it works, let's download the “brice/mysql” image, which you do with the docker pull brice/mysql command. You can see that Docker
downloaded not only the speciﬁed image, but also the images it was built on. With the docker images command, you list the images currently available locally, which
includes the “brice/mysql” image. Launching the container with the -d option to detach from the currently running container, you now have MySQL running in a
container. You can verify that with the docker ps command, which lists containers, rather than images. In the output, you also see the port on which MySQL is listening,
which is the default of 3306.
But, how do you connect to MySQL, knowing that it is running inside a container? Remember that Docker containers get their own network interface. You need to ﬁnd the
IP address and port at which the mysqld server process is listening. The docker inspect <imageId> provides a lot of info, but since all you need is the IP address, you
can just grep for that when inspecting the container by providing its hash docker inspect 5a9005441bb5 | grep IPAddress. Now you can connect with the standard
MySQL CLI client by specifying the host and port options. When you're done with the MySQL server, you can shut it down with docker stop 5a9005441bb5.
It took seven commands to ﬁnd, download and launch a Docker container to get a MySQL server running and shut it down after you're done. In the process, you didn't
have to worry about conﬂicts with installed software, perhaps a different version of MySQL, or dependencies. You used seven different Docker commands: search, pull,
images, run, ps, inspect and stop, but the Docker client actually offers 33 different commands. You can see the full list by running docker help from the command line or
by consulting the on-line manual.
Before exercising Docker in the above example, I mentioned that the client communicates with the dæmon and the Docker Registry via REST-based Web services. That
implies that you can use a local Docker client to interact with a remote dæmon, effectively administering your containers on a remote machine. The APIs for the Docker
dæmon, Registry and Index are nicely documented, illustrated with examples and available on the Docker site (see Resources).
Docker Workﬂow
There are various ways in which Docker can be integrated into the development and deployment process. Let's take a look at a sample workﬂow illustrated in Figure 4. A
developer in our hypothetical company might be running Ubuntu with Docker installed. He might push/pull Docker images to/from the public registry to use as the base
for installing his own code and the company's proprietary software and produce images that he pushes to the company's private registry.
The company's QA environment in this example is running CentOS and Docker. It pulls images from the public and private registries and starts various containers
whenever the environment is updated.
Finally, the company hosts its production environment in the cloud, namely on Amazon Web Services, for scalability and elasticity. Amazon Linux is also running Docker,
which is managing various containers.
Note that all three environments are running different versions of Linux, all of which are compatible with Docker. Moreover, the environments are running various
combinations of containers. However, since each container compartmentalizes its own dependencies, there are no conﬂicts, and all the containers happily coexist.
Figure 4. Sample Software Development Workﬂow Using Docker
It is crucial to understand that Docker promotes an application-centric container model. That is to say, containers should run individual applications or services, rather than
a whole slew of them. Remember that containers are fast and resource-cheap to create and run. Following the single-responsibility principle and running one main process
per container results in loose coupling of the components of your system. With that in mind, let's create your own image from which to launch a container.
Creating a New Docker Image
In the previous example, you interacted with Docker from the command line. However, when creating images, it is far more common to create a “Dockerﬁle” to automate
the build process. Dockerﬁles are simple text ﬁles that describe the build process. You can put a Dockerﬁle under version control and have a perfectly repeatable way of