{
  "metadata": {
    "filename": "temp_a8554558-3dbc-4187-8850-a034bdf3cd1b",
    "extraction_date": "2025-01-28T19:51:24.246804",
    "path": "research_assistant\\data\\temp_a8554558-3dbc-4187-8850-a034bdf3cd1b.pdf"
  },
  "pages": {
    "1": "Docker: Lightweight Linux Containers for Consistent Development and\nDeployment\nDirk Merkel\nAbstract\nTake on “dependency hell” with Docker containers, the lightweight and nimble cousin of VMs. Learn how Docker makes applications portable and isolated by packaging\nthem in containers based on LXC technology.\nImagine being able to package an application along with all of its dependencies easily and then run it smoothly in disparate development, test and production\nenvironments. That is the goal of the open-source Docker project. Although it is still not ofﬁcially production-ready, the latest release (0.7.x at the time of this writing)\nbrought Docker another step closer to realizing this ambitious goal.\nDocker tries to solve the problem of “dependency hell”. Modern applications often are assembled from existing components and rely on other services and applications.\nFor example, your Python application might use PostgreSQL as a data store, Redis for caching and Apache as a Web server. Each of these components comes with its own\nset of dependencies that may conﬂict with those of other components. By packaging each component and its dependencies, Docker solves the following problems:\nConﬂicting dependencies: need to run one Web site on PHP 4.3 and another on PHP 5.5? No problem if you run each version of PHP in a separate Docker container.\nMissing dependencies: installing applications in a new environment is a snap with Docker, because all dependencies are packaged along with the application in a\ncontainer.\nPlatform differences: moving from one distro to another is no longer a problem. If both systems run Docker, the same container will execute without issues.\nDocker: a Little Background\nDocker started life as an open-source project at dotCloud, a cloud-centric platform-as-a-service company, in early 2013. Initially, Docker was a natural extension of the\ntechnology the company had developed to run its cloud business on thousands of servers. It is written in Go, a statically typed programming language developed by\nGoogle with syntax loosely based on C. Fast-forward six to nine months, and the company has hired a new CEO, joined the Linux Foundation, changed its name to\nDocker Inc., and announced that it is shifting its focus to the development of Docker and the Docker ecosystem. As further indication of Docker's popularity, at the time of\nthis writing, it has been starred on GitHub 8,985 times and has been forked 1,304 times. Figure 1 illustrates Docker's rising popularity in Google searches. I predict that\nthe shape of the past 12 months will be dwarfed by the next 12 months as Docker Inc. delivers the ﬁrst version blessed for production deployments of containers and the\ncommunity at large becomes aware of Docker's usefulness.\nFigure 1. Google Trends Graph for “Docker Software” for Past 12 Months\nUnder the Hood\nDocker harnesses some powerful kernel-level technology and puts it at our ﬁngertips. The concept of a container in virtualization has been around for several years, but by\nproviding a simple tool set and a uniﬁed API for managing some kernel-level technologies, such as LXCs (LinuX Containers), cgroups and a copy-on-write ﬁlesystem,\nDocker has created a tool that is greater than the sum of its parts. The result is a potential game-changer for DevOps, system administrators and developers.\nDocker provides tools to make creating and working with containers as easy as possible. Containers sandbox processes from each other. For now, you can think of a\ncontainer as a lightweight equivalent of a virtual machine.\nLinux Containers and LXC, a user-space control package for Linux Containers, constitute the core of Docker. LXC uses kernel-level namespaces to isolate the container\nfrom the host. The user namespace separates the container's and the host's user database, thus ensuring that the container's root user does not have root privileges on the\nhost. The process namespace is responsible for displaying and managing only processes running in the container, not the host. And, the network namespace provides the\ncontainer with its own network device and virtual IP address.\nAnother component of Docker provided by LXC are Control Groups (cgroups). While namespaces are responsible for isolation between host and container, control groups\nimplement resource accounting and limiting. While allowing Docker to limit the resources being consumed by a container, such as memory, disk space and I/O, cgroups\nalso output lots of metrics about these resources. These metrics allow Docker to monitor the resource consumption of the various processes within the containers and make\nsure that each gets only its fair share of the available resources.\nIn addition to the above components, Docker has been using AuFS (Advanced Multi-Layered Uniﬁcation Filesystem) as a ﬁlesystem for containers. AuFS is a layered\nﬁlesystem that can transparently overlay one or more existing ﬁlesystems. When a process needs to modify a ﬁle, AuFS creates a copy of that ﬁle. AuFS is capable of\nmerging multiple layers into a single representation of a ﬁlesystem. This process is called copy-on-write.",
    "2": "The really cool thing is that AuFS allows Docker to use certain images as the basis for containers. For example, you might have a CentOS Linux image that can be used\nas the basis for many different containers. Thanks to AuFS, only one copy of the CentOS image is required, which results in savings of storage and memory, as well as\nfaster deployments of containers.\nAn added beneﬁt of using AuFS is Docker's ability to version container images. Each new version is simply a diff of changes from the previous version, effectively\nkeeping image ﬁles to a minimum. But, it also means that you always have a complete audit trail of what has changed from one version of a container to another.\nTraditionally, Docker has depended on AuFS to provide a copy-on-write storage mechanism. However, the recent addition of a storage driver API is likely to lessen that\ndependence. Initially, there are three storage drivers available: AuFS, VFS and Device-Mapper, which is the result of a collaboration with Red Hat.\nAs of version 0.7, Docker works with all Linux distributions. However, it does not work with most non-Linux operating systems, such as Windows and OS X. The\nrecommended way of using Docker on those OSes is to provision a virtual machine on VirtualBox using Vagrant.\nContainers vs. Other Types of Virtualization\nSo what exactly is a container and how is it different from hypervisor-based virtualization? To put it simply, containers virtualize at the operating system level, whereas\nhypervisor-based solutions virtualize at the hardware level. While the effect is similar, the differences are important and signiﬁcant, which is why I'll spend a little time\nexploring the differences and the resulting differences and trade-offs.\nVirtualization:\nBoth containers and VMs are virtualization tools. On the VM side, a hypervisor makes siloed slices of hardware available. There are generally two types of hypervisors:\n“Type 1” runs directly on the bare metal of the hardware, while “Type 2” runs as an additional layer of software within a guest OS. While the open-source Xen and\nVMware's ESX are examples of Type 1 hypervisors, examples of Type 2 include Oracle's open-source VirtualBox and VMware Server. Although Type 1 is a better\ncandidate for comparison to Docker containers, I don't make a distinction between the two types for the rest of this article.\nContainers, in contrast, make available protected portions of the operating system—they effectively virtualize the operating system. Two containers running on the same\noperating system don't know that they are sharing resources because each has its own abstracted networking layer, processes and so on.\nOperating Systems and Resources:\nSince hypervisor-based virtualization provides access to hardware only, you still need to install an operating system. As a result, there are multiple full-ﬂedged operating\nsystems running, one in each VM, which quickly gobbles up resources on the server, such as RAM, CPU and bandwidth.\nContainers piggyback on an already running operating system as their host environment. They merely execute in spaces that are isolated from each other and from certain\nparts of the host OS. This has two signiﬁcant beneﬁts. First, resource utilization is much more efﬁcient. If a container is not executing anything, it is not using up\nresources, and containers can call upon their host OS to satisfy some or all of their dependencies. Second, containers are cheap and therefore fast to create and destroy.\nThere is no need to boot and shut down a whole OS. Instead, a container merely has to terminate the processes running in its isolated space. Consequently, starting and\nstopping a container is more akin to starting and quitting an application, and is just as fast.\nBoth types of virtualization and containers are illustrated in Figure 2.\nFigure 2. VMs vs. Containers\nIsolation for Performance and Security:\nProcesses executing in a Docker container are isolated from processes running on the host OS or in other Docker containers. Nevertheless, all processes are executing in\nthe same kernel. Docker leverages LXC to provide separate namespaces for containers, a technology that has been present in Linux kernels for 5+ years and is considered\nfairly mature. It also uses Control Groups, which have been in the Linux kernel even longer, to implement resource auditing and limiting.\nThe Docker dæmon itself also poses a potential attack vector because it currently runs with root privileges. Improvements to both LXC and Docker should allow\ncontainers to run without root privileges and to execute the Docker dæmon under a different system user.\nAlthough the type of isolation provided is overall quite strong, it is arguably not as strong as what can be enforced by virtual machines at the hypervisor level. If the kernel\ngoes down, so do all the containers. The other area where VMs have the advantage is their maturity and widespread adoption in production environments. VMs have been\nhardened and proven themselves in many different high-availability environments. In comparison, Docker and its supporting technologies have not seen nearly as much\naction. Docker in particular is undergoing massive changes every day, and we all know that change is the enemy of security.\nDocker and VMs—Frenemies:\nNow that I've spent all this time comparing Docker and VMs, it's time to acknowledge that these two technologies can actually complement each other. Docker runs just\nﬁne on already-virtualized environments. You obviously don't want to incur the cost of encapsulating each application or component in a separate VM, but given a Linux\nVM, you can easily deploy Docker containers on it. That is why it should not come as a surprise that the ofﬁcially supported way of using Docker on non-Linux systems,\nsuch as OS X and Windows, is to install a Precise64 base Ubuntu virtual machine with the help of Vagrant. Simple detailed instructions are provided on the\nhttp://www.docker.io site.",
    "3": "The bottom line is that virtualization and containers exhibit some similarities. Initially, it helps to think of containers as very lightweight virtualization. However, as you\nspend more time with containers, you come to understand the subtle but important differences. Docker does a nice job of harnessing the beneﬁts of containerization for a\nfocused purpose, namely the lightweight packaging and deployment of applications.\nDocker Repositories\nOne of Docker's killer features is the ability to ﬁnd, download and start container images that were created by other developers quickly. The place where images are stored\nis called a registry, and Docker Inc. offers a public registry also called the Central Index. You can think of the registry along with the Docker client as the equivalent of\nNode's NPM, Perl's CPAN or Ruby's RubyGems.\nIn addition to various base images, which you can use to create your own Docker containers, the public Docker Registry features images of ready-to-run software,\nincluding databases, content management systems, development environments, Web servers and so on. While the Docker command-line client searches the public Registry\nby default, it is also possible to maintain private registries. This is a great option for distributing images with proprietary code or components internally to your company.\nPushing images to the registry is just as easy as downloading. It requires you to create an account, but that is free as well. Lastly, Docker Inc.'s registry has a Web-based\ninterface for searching for, reading about, commenting on and recommending (aka “starring”) images. It is ridiculously easy to use, and I encourage you to click the link in\nthe Resources section of this article and start exploring.\nHands-On with Docker\nDocker consists of a single binary that can be run in one of three different ways. First, it can run as a dæmon to manage the containers. The dæmon exposes a REST-based\nAPI that can be accessed locally or remotely. A growing number of client libraries are available to interact with the dæmon's API, including Ruby, Python, JavaScript\n(Angular and Node), Erlang, Go and PHP.\nThe client libraries are great for accessing the dæmon programmatically, but the more common use case is to issue instructions from the command line, which is the\nsecond way the Docker binary can be used, namely as a command-line client to the REST-based dæmon.\nThird, the Docker binary functions as a client to remote repositories of images. Tagged images that make up the ﬁlesystem for a container are called repositories. Users can\npull images provided by others and share their own images by pushing them to the registry. Registries are used to collect, list and organize repositories.\nLet's see all three ways of running the docker executable in action. In this example, you'll search the Docker repository for a MySQL image. Once you ﬁnd an image you\nlike, you'll download it, and tell the Docker dæmon to run the command (MySQL). You'll do all of this from the command line.\nFigure 3. Pulling a Docker Image and Launching a Container",
    "4": "Start by issuing the docker search mysql command, which then displays a list of images in the public Docker registry that match the keyword “mysql”. For no particular\nreason other than I know it works, let's download the “brice/mysql” image, which you do with the docker pull brice/mysql command. You can see that Docker\ndownloaded not only the speciﬁed image, but also the images it was built on. With the docker images command, you list the images currently available locally, which\nincludes the “brice/mysql” image. Launching the container with the -d option to detach from the currently running container, you now have MySQL running in a\ncontainer. You can verify that with the docker ps command, which lists containers, rather than images. In the output, you also see the port on which MySQL is listening,\nwhich is the default of 3306.\nBut, how do you connect to MySQL, knowing that it is running inside a container? Remember that Docker containers get their own network interface. You need to ﬁnd the\nIP address and port at which the mysqld server process is listening. The docker inspect <imageId> provides a lot of info, but since all you need is the IP address, you\ncan just grep for that when inspecting the container by providing its hash docker inspect 5a9005441bb5 | grep IPAddress. Now you can connect with the standard\nMySQL CLI client by specifying the host and port options. When you're done with the MySQL server, you can shut it down with docker stop 5a9005441bb5.\nIt took seven commands to ﬁnd, download and launch a Docker container to get a MySQL server running and shut it down after you're done. In the process, you didn't\nhave to worry about conﬂicts with installed software, perhaps a different version of MySQL, or dependencies. You used seven different Docker commands: search, pull,\nimages, run, ps, inspect and stop, but the Docker client actually offers 33 different commands. You can see the full list by running docker help from the command line or\nby consulting the on-line manual.\nBefore exercising Docker in the above example, I mentioned that the client communicates with the dæmon and the Docker Registry via REST-based Web services. That\nimplies that you can use a local Docker client to interact with a remote dæmon, effectively administering your containers on a remote machine. The APIs for the Docker\ndæmon, Registry and Index are nicely documented, illustrated with examples and available on the Docker site (see Resources).\nDocker Workﬂow\nThere are various ways in which Docker can be integrated into the development and deployment process. Let's take a look at a sample workﬂow illustrated in Figure 4. A\ndeveloper in our hypothetical company might be running Ubuntu with Docker installed. He might push/pull Docker images to/from the public registry to use as the base\nfor installing his own code and the company's proprietary software and produce images that he pushes to the company's private registry.\nThe company's QA environment in this example is running CentOS and Docker. It pulls images from the public and private registries and starts various containers\nwhenever the environment is updated.\nFinally, the company hosts its production environment in the cloud, namely on Amazon Web Services, for scalability and elasticity. Amazon Linux is also running Docker,\nwhich is managing various containers.\nNote that all three environments are running different versions of Linux, all of which are compatible with Docker. Moreover, the environments are running various\ncombinations of containers. However, since each container compartmentalizes its own dependencies, there are no conﬂicts, and all the containers happily coexist.\nFigure 4. Sample Software Development Workﬂow Using Docker\nIt is crucial to understand that Docker promotes an application-centric container model. That is to say, containers should run individual applications or services, rather than\na whole slew of them. Remember that containers are fast and resource-cheap to create and run. Following the single-responsibility principle and running one main process\nper container results in loose coupling of the components of your system. With that in mind, let's create your own image from which to launch a container.\nCreating a New Docker Image\nIn the previous example, you interacted with Docker from the command line. However, when creating images, it is far more common to create a “Dockerﬁle” to automate\nthe build process. Dockerﬁles are simple text ﬁles that describe the build process. You can put a Dockerﬁle under version control and have a perfectly repeatable way of",
    "5": "creating an image.\nFor the next example, please refer to the “PHP Box” Dockerﬁle (Listing 1). Garrick, shrink listing 1.\nListing 1. PHP Box11600l1.qrk\n # PHP Box # # VERSION 1.0 # use centos base image FROM centos:6.4 # specify the maintainer MAINTAINER Dirk Merkel, dmerkel@vivantech.com # up\nLet's take a closer look at what's going on in this Dockerﬁle. The syntax of a Dockerﬁle is a command keyword followed by that command's argument(s). By convention,\ncommand keywords are capitalized. Comments start with a pound character.\nThe FROM keyword indicates which image to use as a base. This must be the ﬁrst instruction in the ﬁle. In this case, you will build on top of the latest CentOS base\nimage. The MAINTAINER instruction obviously lists the person who maintains the Dockerﬁle. The RUN instruction executes a command and commits the resulting image,\nthus creating a new layer. The RUN commands in the Dockerﬁle fetch conﬁguration ﬁles for additional repositories and then use Yum to install curl, git, wget, unzip, httpd,\nphp-mysql and yum-utils. I could have combined the yum install commands into a single RUN instruction to avoid successive commits.\nThe EXPOSE instruction then exposes port 80, which is the port on which Apache will be listening when you start the container.\nFinally, the CMD instruction will provide the default command to run when the container is being launched. Associating a single process with the launch of the container\nallows you to treat a container as a command.\nTyping docker build -t php_box . on the command line will now tell Docker to start the build process using the Dockerﬁle in the current working directory. The\nresulting image will be tagged “php_box”, which will make it easier to refer to and identify the image later.\nThe build process downloads the base image and then installs Apache httpd along with all dependencies. Upon completion, it returns a hash identifying the newly created\nimage. Similar to the MySQL container you launched earlier, you can run the Apache and PHP image using the “php_box” tag with the following command line: docker\nrun -d -t php_box.\nLet's ﬁnish with a quick example that illustrates how easy it is to layer on top of an existing image to create a new one:\n # MyApp # # VERSION  1.0 # use php_box base image FROM php_box # specify the maintainer MAINTAINER Dirk Merkel, dmerkel@vivantech.com # put m\nThis second Dockerﬁle is shorter than the ﬁrst and really contains only two interesting instructions. First, you specify the “php_box” image as a starting point using the\nFROM instruction. Second, you copy a local directory to the image with the ADD instruction. In this case, it is a PHP project that is being copied to Apache's\nDOCUMENT_ROOT folder on the images. The result is that the site will be served by default when you launch the image.\nConclusion\nDocker's prospect of lightweight packaging and deploying of applications and dependencies is an exciting one, and it is quickly being adopted by the Linux community\nand is making its way into production environments. For example, Red Hat announced in December support for Docker in the upcoming Red Hat Enterprise Linux 7.\nHowever, Docker is still a young project and is growing at breakneck speed. It is going to be exciting to watch as the project approaches its 1.0 release, which is supposed\nto be the ﬁrst version ofﬁcially sanctioned for production environments. Docker relies on established technologies, some of which have been around for more than a\ndecade, but that doesn't make it any less revolutionary. Hopefully this article provided you with enough information and inspiration to download Docker and experiment\nwith it yourself.\nDocker Update11600s1.qrk\nAs this article was being published, the Docker team announced the release of version 0.8. This latest deliverable adds support for Mac OS X consisting of two\ncomponents. While the client runs natively on OS X, the Docker dæmon runs inside a lightweight VirtualBox-based VM that is easily managed with boot2docker, the\nincluded command-line client. This approach is necessary because the underlying technologies, such as LXC and name spaces, simply are not supported by OS X. I think\nwe can expect a similar solution for other platforms, including Windows.\nVersion 0.8 also introduces several new builder features and experimental support for BTRFS (B-Tree File System). BTRFS is another copy-on-write ﬁlesystem, and the\nBTRFS storage driver is positioned as an alternative to the AuFS driver.\nMost notably, Docker 0.8 brings with it many bug ﬁxes and performance enhancements. This overall commitment to quality signals an effort by the Docker team to\nproduce a version 1.0 that is ready to be used in production environments. With the team committing to a monthly release cycle, we can look forward to the 1.0 release in\nthe April to May timeframe.\nResources11600s2.qrk\nMain Docker Site: https://www.docker.io\nDocker Registry: https://index.docker.io\nDocker Registry API: http://docs.docker.io/en/latest/api/registry_api\nDocker Index API: http://docs.docker.io/en/latest/api/index_api\nDocker Remote API: http://docs.docker.io/en/latest/api/docker_remote_api\n11600s3.qrk\nSend comments or feedback via http://www.linuxjournal.com/contact or to <ljeditor@linuxjournal.com>."
  },
  "tables": {},
  "images": {
    "1": [
      {
        "filename": "page_1_image_1.png",
        "path": "output\\images\\temp_a8554558-3dbc-4187-8850-a034bdf3cd1b\\page_1_image_1.png",
        "extraction_date": "2025-01-28T19:51:25.940832",
        "page_number": 1,
        "image_index": 1,
        "width": 799,
        "height": 287
      }
    ],
    "2": [
      {
        "filename": "page_2_image_1.png",
        "path": "output\\images\\temp_a8554558-3dbc-4187-8850-a034bdf3cd1b\\page_2_image_1.png",
        "extraction_date": "2025-01-28T19:51:25.955133",
        "page_number": 2,
        "image_index": 1,
        "width": 799,
        "height": 287
      }
    ],
    "3": [
      {
        "filename": "page_3_image_1.png",
        "path": "output\\images\\temp_a8554558-3dbc-4187-8850-a034bdf3cd1b\\page_3_image_1.png",
        "extraction_date": "2025-01-28T19:51:26.045706",
        "page_number": 3,
        "image_index": 1,
        "width": 846,
        "height": 816
      }
    ],
    "4": [
      {
        "filename": "page_4_image_1.png",
        "path": "output\\images\\temp_a8554558-3dbc-4187-8850-a034bdf3cd1b\\page_4_image_1.png",
        "extraction_date": "2025-01-28T19:51:26.116102",
        "page_number": 4,
        "image_index": 1,
        "width": 695,
        "height": 600
      }
    ]
  }
}